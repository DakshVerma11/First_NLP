{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MCdtjT-bly"
      },
      "source": [
        "The high level steps to prepare text to be used in a machine learning model are:\n",
        "\n",
        "1.   Tokenize the words to get numerical values for them\n",
        "2.   Create numerical sequences of the sentences\n",
        "3.   Adjust the sequences to all be the same length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJsd8KslUn7j"
      },
      "source": [
        "## Import the classes you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ZxQf11OUtQI"
      },
      "outputs": [],
      "source": [
        "# Import Tokenizer and pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MeEgRq4WX0v"
      },
      "source": [
        "## Write some sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PwM7IP2lTr7T",
        "outputId": "9b96ae77-4095-43c4-d839-00bf6cb09dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love eating pizza on weekends', 'Do you enjoy playing soccer?', 'My cat naps all day', 'Your garden looks beautiful with roses', 'Fresh apples are my go-to snack', 'The movie was thrilling and suspenseful', 'Hiking in the mountains is refreshing', 'She reads a book every week', 'The sunset over the ocean is breathtaking', 'Baking cookies is a fun activity', 'I often ride my bike to work', 'Your painting is quite impressive', 'Traveling to new countries excites me', 'He practices the piano daily', 'Our team won the championship game', 'Coffee in the morning wakes me up', 'The new restaurant downtown is fantastic', 'Learning new languages is beneficial', 'The thunderstorm scared my cat', 'Gardening is a peaceful hobby', 'They enjoy kayaking on the lake', 'His artwork is displayed in the gallery', 'I plan to visit the museum tomorrow', 'Your car is really fast', 'The concert last night was amazing', 'I need to buy groceries today', 'She volunteers at the animal shelter', 'The library is a quiet place to study', 'Their wedding was beautiful and touching', 'Yoga helps me relax and unwind', 'My favorite food is ice cream', 'do you like ice cream too?', 'My dog likes ice cream!', 'your favorite flavor of icecream is chocolate', \"chocolate isn't good for dogs\", 'your dog, your cat, and your parrot prefer broccoli']\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    'I love eating pizza on weekends',\n",
        "    'Do you enjoy playing soccer?',\n",
        "    'My cat naps all day',\n",
        "    'Your garden looks beautiful with roses',\n",
        "    'Fresh apples are my go-to snack',\n",
        "    'The movie was thrilling and suspenseful',\n",
        "    'Hiking in the mountains is refreshing',\n",
        "    'She reads a book every week',\n",
        "    'The sunset over the ocean is breathtaking',\n",
        "    'Baking cookies is a fun activity',\n",
        "    'I often ride my bike to work',\n",
        "    'Your painting is quite impressive',\n",
        "    'Traveling to new countries excites me',\n",
        "    'He practices the piano daily',\n",
        "    'Our team won the championship game',\n",
        "    'Coffee in the morning wakes me up',\n",
        "    'The new restaurant downtown is fantastic',\n",
        "    'Learning new languages is beneficial',\n",
        "    'The thunderstorm scared my cat',\n",
        "    'Gardening is a peaceful hobby',\n",
        "    'They enjoy kayaking on the lake',\n",
        "    'His artwork is displayed in the gallery',\n",
        "    'I plan to visit the museum tomorrow',\n",
        "    'Your car is really fast',\n",
        "    'The concert last night was amazing',\n",
        "    'I need to buy groceries today',\n",
        "    'She volunteers at the animal shelter',\n",
        "    'The library is a quiet place to study',\n",
        "    'Their wedding was beautiful and touching',\n",
        "    'Yoga helps me relax and unwind',\n",
        "    'My favorite food is ice cream',\n",
        "    'do you like ice cream too?',\n",
        "    'My dog likes ice cream!',\n",
        "    \"your favorite flavor of icecream is chocolate\",\n",
        "    \"chocolate isn't good for dogs\",\n",
        "    \"your dog, your cat, and your parrot prefer broccoli\"\n",
        "]\n",
        "print(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaRa9opNWmA7"
      },
      "source": [
        "## Create the Tokenizer and define an out of vocabulary token\n",
        "When creating the Tokenizer, you can specify the max number of words in the dictionary. You can also specify a token to represent words that are out of the vocabulary (OOV), in other words, that are not in the dictionary. This OOV token will be used when you create sequences for sentences that contain words that are not in the word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P7wuOJaBWiHZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7nILrhKXPge"
      },
      "source": [
        "## Tokenize the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YXooiuwrXROU",
        "outputId": "d794686b-d778-4730-cf0d-adfdc8f4a253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'the': 2, 'is': 3, 'your': 4, 'my': 5, 'to': 6, 'i': 7, 'and': 8, 'a': 9, 'cat': 10, 'was': 11, 'in': 12, 'new': 13, 'me': 14, 'ice': 15, 'cream': 16, 'on': 17, 'do': 18, 'you': 19, 'enjoy': 20, 'beautiful': 21, 'she': 22, 'favorite': 23, 'dog': 24, 'chocolate': 25, 'love': 26, 'eating': 27, 'pizza': 28, 'weekends': 29, 'playing': 30, 'soccer': 31, 'naps': 32, 'all': 33, 'day': 34, 'garden': 35, 'looks': 36, 'with': 37, 'roses': 38, 'fresh': 39, 'apples': 40, 'are': 41, 'go': 42, 'snack': 43, 'movie': 44, 'thrilling': 45, 'suspenseful': 46, 'hiking': 47, 'mountains': 48, 'refreshing': 49, 'reads': 50, 'book': 51, 'every': 52, 'week': 53, 'sunset': 54, 'over': 55, 'ocean': 56, 'breathtaking': 57, 'baking': 58, 'cookies': 59, 'fun': 60, 'activity': 61, 'often': 62, 'ride': 63, 'bike': 64, 'work': 65, 'painting': 66, 'quite': 67, 'impressive': 68, 'traveling': 69, 'countries': 70, 'excites': 71, 'he': 72, 'practices': 73, 'piano': 74, 'daily': 75, 'our': 76, 'team': 77, 'won': 78, 'championship': 79, 'game': 80, 'coffee': 81, 'morning': 82, 'wakes': 83, 'up': 84, 'restaurant': 85, 'downtown': 86, 'fantastic': 87, 'learning': 88, 'languages': 89, 'beneficial': 90, 'thunderstorm': 91, 'scared': 92, 'gardening': 93, 'peaceful': 94, 'hobby': 95, 'they': 96, 'kayaking': 97, 'lake': 98, 'his': 99, 'artwork': 100, 'displayed': 101, 'gallery': 102, 'plan': 103, 'visit': 104, 'museum': 105, 'tomorrow': 106, 'car': 107, 'really': 108, 'fast': 109, 'concert': 110, 'last': 111, 'night': 112, 'amazing': 113, 'need': 114, 'buy': 115, 'groceries': 116, 'today': 117, 'volunteers': 118, 'at': 119, 'animal': 120, 'shelter': 121, 'library': 122, 'quiet': 123, 'place': 124, 'study': 125, 'their': 126, 'wedding': 127, 'touching': 128, 'yoga': 129, 'helps': 130, 'relax': 131, 'unwind': 132, 'food': 133, 'like': 134, 'too': 135, 'likes': 136, 'flavor': 137, 'of': 138, 'icecream': 139, \"isn't\": 140, 'good': 141, 'for': 142, 'dogs': 143, 'parrot': 144, 'prefer': 145, 'broccoli': 146}\n"
          ]
        }
      ],
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U-oe201Xm7T"
      },
      "source": [
        "## Turn sentences into sequences\n",
        "\n",
        "Each word now has a unique number in the word index.  However, words in a sentence are in a specific order. You can't just randomly mix up words and have the outcome be a sentence.\n",
        "\n",
        "For example, although \"chocolate isn't good for dogs\" is a perfectly fine sentence, \"dogs isn't for chocolate good\" does not make sense as a sentence.\n",
        "\n",
        "So the next step to representing text in a way that can be meaningfully used by machine learning programs is to create numerical sequences that represent the sentences in the text.\n",
        "\n",
        "Each sentence will be converted into a sequence where each word is replaced by its number in the word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "70l5x1XRXoV4",
        "outputId": "7293124e-1b39-488e-da89-53d7bc2e6b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7, 26, 27, 28, 17, 29], [18, 19, 20, 30, 31], [5, 10, 32, 33, 34], [4, 35, 36, 21, 37, 38], [39, 40, 41, 5, 42, 6, 43], [2, 44, 11, 45, 8, 46], [47, 12, 2, 48, 3, 49], [22, 50, 9, 51, 52, 53], [2, 54, 55, 2, 56, 3, 57], [58, 59, 3, 9, 60, 61], [7, 62, 63, 5, 64, 6, 65], [4, 66, 3, 67, 68], [69, 6, 13, 70, 71, 14], [72, 73, 2, 74, 75], [76, 77, 78, 2, 79, 80], [81, 12, 2, 82, 83, 14, 84], [2, 13, 85, 86, 3, 87], [88, 13, 89, 3, 90], [2, 91, 92, 5, 10], [93, 3, 9, 94, 95], [96, 20, 97, 17, 2, 98], [99, 1, 3, 1, 12, 2, 1], [7, 1, 6, 1, 2, 1, 1], [4, 1, 3, 1, 1], [2, 1, 1, 1, 11, 1], [7, 1, 6, 1, 1, 1], [22, 1, 1, 2, 1, 1], [2, 1, 3, 9, 1, 1, 6, 1], [1, 1, 11, 21, 8, 1], [1, 1, 14, 1, 8, 1], [5, 23, 1, 3, 15, 16], [18, 19, 1, 15, 16, 1], [5, 24, 1, 15, 16], [4, 23, 1, 1, 1, 3, 25], [25, 1, 1, 1, 1], [4, 24, 4, 10, 8, 4, 1, 1, 1]]\n"
          ]
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print (sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcFghvQ34cZK"
      },
      "source": [
        "## Make the sequences all the same length\n",
        "\n",
        "Later, when you feed the sequences into a neural network to train a model, the sequences all need to be uniform in size. Currently the sequences have varied lengths, so the next step is to make them all be the same size, either by padding them with zeros and/or truncating them.\n",
        "\n",
        "Use f.keras.preprocessing.sequence.pad_sequences to add zeros to the sequences to make them all be the same length. By default, the padding goes at the start of the sequences, but you can specify to pad at the end.\n",
        "\n",
        "You can optionally specify the maximum length to pad the sequences to. Sequences that are longer than the specified max length will be truncated. By default, sequences are truncated from the beginning of the sequence, but you can specify to truncate from the end.\n",
        "\n",
        "If you don't provide the max length, then the sequences are padded to match the length of the longest sentence.\n",
        "\n",
        "For all the options when padding and truncating sequences, see https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r0m_nqHg4gqu",
        "outputId": "fae13611-7e41-4cf6-9723-beccbeca0799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'the': 2, 'is': 3, 'your': 4, 'my': 5, 'to': 6, 'i': 7, 'and': 8, 'a': 9, 'cat': 10, 'was': 11, 'in': 12, 'new': 13, 'me': 14, 'ice': 15, 'cream': 16, 'on': 17, 'do': 18, 'you': 19, 'enjoy': 20, 'beautiful': 21, 'she': 22, 'favorite': 23, 'dog': 24, 'chocolate': 25, 'love': 26, 'eating': 27, 'pizza': 28, 'weekends': 29, 'playing': 30, 'soccer': 31, 'naps': 32, 'all': 33, 'day': 34, 'garden': 35, 'looks': 36, 'with': 37, 'roses': 38, 'fresh': 39, 'apples': 40, 'are': 41, 'go': 42, 'snack': 43, 'movie': 44, 'thrilling': 45, 'suspenseful': 46, 'hiking': 47, 'mountains': 48, 'refreshing': 49, 'reads': 50, 'book': 51, 'every': 52, 'week': 53, 'sunset': 54, 'over': 55, 'ocean': 56, 'breathtaking': 57, 'baking': 58, 'cookies': 59, 'fun': 60, 'activity': 61, 'often': 62, 'ride': 63, 'bike': 64, 'work': 65, 'painting': 66, 'quite': 67, 'impressive': 68, 'traveling': 69, 'countries': 70, 'excites': 71, 'he': 72, 'practices': 73, 'piano': 74, 'daily': 75, 'our': 76, 'team': 77, 'won': 78, 'championship': 79, 'game': 80, 'coffee': 81, 'morning': 82, 'wakes': 83, 'up': 84, 'restaurant': 85, 'downtown': 86, 'fantastic': 87, 'learning': 88, 'languages': 89, 'beneficial': 90, 'thunderstorm': 91, 'scared': 92, 'gardening': 93, 'peaceful': 94, 'hobby': 95, 'they': 96, 'kayaking': 97, 'lake': 98, 'his': 99, 'artwork': 100, 'displayed': 101, 'gallery': 102, 'plan': 103, 'visit': 104, 'museum': 105, 'tomorrow': 106, 'car': 107, 'really': 108, 'fast': 109, 'concert': 110, 'last': 111, 'night': 112, 'amazing': 113, 'need': 114, 'buy': 115, 'groceries': 116, 'today': 117, 'volunteers': 118, 'at': 119, 'animal': 120, 'shelter': 121, 'library': 122, 'quiet': 123, 'place': 124, 'study': 125, 'their': 126, 'wedding': 127, 'touching': 128, 'yoga': 129, 'helps': 130, 'relax': 131, 'unwind': 132, 'food': 133, 'like': 134, 'too': 135, 'likes': 136, 'flavor': 137, 'of': 138, 'icecream': 139, \"isn't\": 140, 'good': 141, 'for': 142, 'dogs': 143, 'parrot': 144, 'prefer': 145, 'broccoli': 146}\n",
            "\n",
            "Sequences =  [[7, 26, 27, 28, 17, 29], [18, 19, 20, 30, 31], [5, 10, 32, 33, 34], [4, 35, 36, 21, 37, 38], [39, 40, 41, 5, 42, 6, 43], [2, 44, 11, 45, 8, 46], [47, 12, 2, 48, 3, 49], [22, 50, 9, 51, 52, 53], [2, 54, 55, 2, 56, 3, 57], [58, 59, 3, 9, 60, 61], [7, 62, 63, 5, 64, 6, 65], [4, 66, 3, 67, 68], [69, 6, 13, 70, 71, 14], [72, 73, 2, 74, 75], [76, 77, 78, 2, 79, 80], [81, 12, 2, 82, 83, 14, 84], [2, 13, 85, 86, 3, 87], [88, 13, 89, 3, 90], [2, 91, 92, 5, 10], [93, 3, 9, 94, 95], [96, 20, 97, 17, 2, 98], [99, 1, 3, 1, 12, 2, 1], [7, 1, 6, 1, 2, 1, 1], [4, 1, 3, 1, 1], [2, 1, 1, 1, 11, 1], [7, 1, 6, 1, 1, 1], [22, 1, 1, 2, 1, 1], [2, 1, 3, 9, 1, 1, 6, 1], [1, 1, 11, 21, 8, 1], [1, 1, 14, 1, 8, 1], [5, 23, 1, 3, 15, 16], [18, 19, 1, 15, 16, 1], [5, 24, 1, 15, 16], [4, 23, 1, 1, 1, 3, 25], [25, 1, 1, 1, 1], [4, 24, 4, 10, 8, 4, 1, 1, 1]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 0  0  0  7 26 27 28 17 29]\n",
            " [ 0  0  0  0 18 19 20 30 31]\n",
            " [ 0  0  0  0  5 10 32 33 34]\n",
            " [ 0  0  0  4 35 36 21 37 38]\n",
            " [ 0  0 39 40 41  5 42  6 43]\n",
            " [ 0  0  0  2 44 11 45  8 46]\n",
            " [ 0  0  0 47 12  2 48  3 49]\n",
            " [ 0  0  0 22 50  9 51 52 53]\n",
            " [ 0  0  2 54 55  2 56  3 57]\n",
            " [ 0  0  0 58 59  3  9 60 61]\n",
            " [ 0  0  7 62 63  5 64  6 65]\n",
            " [ 0  0  0  0  4 66  3 67 68]\n",
            " [ 0  0  0 69  6 13 70 71 14]\n",
            " [ 0  0  0  0 72 73  2 74 75]\n",
            " [ 0  0  0 76 77 78  2 79 80]\n",
            " [ 0  0 81 12  2 82 83 14 84]\n",
            " [ 0  0  0  2 13 85 86  3 87]\n",
            " [ 0  0  0  0 88 13 89  3 90]\n",
            " [ 0  0  0  0  2 91 92  5 10]\n",
            " [ 0  0  0  0 93  3  9 94 95]\n",
            " [ 0  0  0 96 20 97 17  2 98]\n",
            " [ 0  0 99  1  3  1 12  2  1]\n",
            " [ 0  0  7  1  6  1  2  1  1]\n",
            " [ 0  0  0  0  4  1  3  1  1]\n",
            " [ 0  0  0  2  1  1  1 11  1]\n",
            " [ 0  0  0  7  1  6  1  1  1]\n",
            " [ 0  0  0 22  1  1  2  1  1]\n",
            " [ 0  2  1  3  9  1  1  6  1]\n",
            " [ 0  0  0  1  1 11 21  8  1]\n",
            " [ 0  0  0  1  1 14  1  8  1]\n",
            " [ 0  0  0  5 23  1  3 15 16]\n",
            " [ 0  0  0 18 19  1 15 16  1]\n",
            " [ 0  0  0  0  5 24  1 15 16]\n",
            " [ 0  0  4 23  1  1  1  3 25]\n",
            " [ 0  0  0  0 25  1  1  1  1]\n",
            " [ 4 24  4 10  8  4  1  1  1]]\n"
          ]
        }
      ],
      "source": [
        "padded = pad_sequences(sequences)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VzbGtYWQ6Ofo",
        "outputId": "07e72c96-9d38-4f95-9c68-eda204a3883f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  7 26 27 28 17 29]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 18 19 20 30 31]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  5 10 32 33 34]\n",
            " [ 0  0  0  0  0  0  0  0  0  4 35 36 21 37 38]\n",
            " [ 0  0  0  0  0  0  0  0 39 40 41  5 42  6 43]\n",
            " [ 0  0  0  0  0  0  0  0  0  2 44 11 45  8 46]\n",
            " [ 0  0  0  0  0  0  0  0  0 47 12  2 48  3 49]\n",
            " [ 0  0  0  0  0  0  0  0  0 22 50  9 51 52 53]\n",
            " [ 0  0  0  0  0  0  0  0  2 54 55  2 56  3 57]\n",
            " [ 0  0  0  0  0  0  0  0  0 58 59  3  9 60 61]\n",
            " [ 0  0  0  0  0  0  0  0  7 62 63  5 64  6 65]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  4 66  3 67 68]\n",
            " [ 0  0  0  0  0  0  0  0  0 69  6 13 70 71 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 72 73  2 74 75]\n",
            " [ 0  0  0  0  0  0  0  0  0 76 77 78  2 79 80]\n",
            " [ 0  0  0  0  0  0  0  0 81 12  2 82 83 14 84]\n",
            " [ 0  0  0  0  0  0  0  0  0  2 13 85 86  3 87]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 88 13 89  3 90]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  2 91 92  5 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 93  3  9 94 95]\n",
            " [ 0  0  0  0  0  0  0  0  0 96 20 97 17  2 98]\n",
            " [ 0  0  0  0  0  0  0  0 99  1  3  1 12  2  1]\n",
            " [ 0  0  0  0  0  0  0  0  7  1  6  1  2  1  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  4  1  3  1  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  2  1  1  1 11  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  7  1  6  1  1  1]\n",
            " [ 0  0  0  0  0  0  0  0  0 22  1  1  2  1  1]\n",
            " [ 0  0  0  0  0  0  0  2  1  3  9  1  1  6  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  1 11 21  8  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  1  1 14  1  8  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  5 23  1  3 15 16]\n",
            " [ 0  0  0  0  0  0  0  0  0 18 19  1 15 16  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  5 24  1 15 16]\n",
            " [ 0  0  0  0  0  0  0  0  4 23  1  1  1  3 25]\n",
            " [ 0  0  0  0  0  0  0  0  0  0 25  1  1  1  1]\n",
            " [ 0  0  0  0  0  0  4 24  4 10  8  4  1  1  1]]\n"
          ]
        }
      ],
      "source": [
        "# Specify a max length for the padded sequences\n",
        "padded = pad_sequences(sequences, maxlen=15)\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HzkbHi0B64w8",
        "outputId": "ed820fc1-995e-4e44-c5a8-806d39a35f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 7 26 27 28 17 29  0  0  0  0  0  0  0  0  0]\n",
            " [18 19 20 30 31  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 5 10 32 33 34  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4 35 36 21 37 38  0  0  0  0  0  0  0  0  0]\n",
            " [39 40 41  5 42  6 43  0  0  0  0  0  0  0  0]\n",
            " [ 2 44 11 45  8 46  0  0  0  0  0  0  0  0  0]\n",
            " [47 12  2 48  3 49  0  0  0  0  0  0  0  0  0]\n",
            " [22 50  9 51 52 53  0  0  0  0  0  0  0  0  0]\n",
            " [ 2 54 55  2 56  3 57  0  0  0  0  0  0  0  0]\n",
            " [58 59  3  9 60 61  0  0  0  0  0  0  0  0  0]\n",
            " [ 7 62 63  5 64  6 65  0  0  0  0  0  0  0  0]\n",
            " [ 4 66  3 67 68  0  0  0  0  0  0  0  0  0  0]\n",
            " [69  6 13 70 71 14  0  0  0  0  0  0  0  0  0]\n",
            " [72 73  2 74 75  0  0  0  0  0  0  0  0  0  0]\n",
            " [76 77 78  2 79 80  0  0  0  0  0  0  0  0  0]\n",
            " [81 12  2 82 83 14 84  0  0  0  0  0  0  0  0]\n",
            " [ 2 13 85 86  3 87  0  0  0  0  0  0  0  0  0]\n",
            " [88 13 89  3 90  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2 91 92  5 10  0  0  0  0  0  0  0  0  0  0]\n",
            " [93  3  9 94 95  0  0  0  0  0  0  0  0  0  0]\n",
            " [96 20 97 17  2 98  0  0  0  0  0  0  0  0  0]\n",
            " [99  1  3  1 12  2  1  0  0  0  0  0  0  0  0]\n",
            " [ 7  1  6  1  2  1  1  0  0  0  0  0  0  0  0]\n",
            " [ 4  1  3  1  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  1  1  1 11  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 7  1  6  1  1  1  0  0  0  0  0  0  0  0  0]\n",
            " [22  1  1  2  1  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  1  3  9  1  1  6  1  0  0  0  0  0  0  0]\n",
            " [ 1  1 11 21  8  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  1 14  1  8  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 5 23  1  3 15 16  0  0  0  0  0  0  0  0  0]\n",
            " [18 19  1 15 16  1  0  0  0  0  0  0  0  0  0]\n",
            " [ 5 24  1 15 16  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4 23  1  1  1  3 25  0  0  0  0  0  0  0  0]\n",
            " [25  1  1  1  1  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 4 24  4 10  8  4  1  1  1  0  0  0  0  0  0]]\n"
          ]
        }
      ],
      "source": [
        "# Put the padding at the end of the sequences\n",
        "padded = pad_sequences(sequences, maxlen=15, padding=\"post\")\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OLHheI477okX",
        "outputId": "22049f2e-ffb4-4267-e58f-eb4ac17fe5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[28 17 29]\n",
            " [20 30 31]\n",
            " [32 33 34]\n",
            " [21 37 38]\n",
            " [42  6 43]\n",
            " [45  8 46]\n",
            " [48  3 49]\n",
            " [51 52 53]\n",
            " [56  3 57]\n",
            " [ 9 60 61]\n",
            " [64  6 65]\n",
            " [ 3 67 68]\n",
            " [70 71 14]\n",
            " [ 2 74 75]\n",
            " [ 2 79 80]\n",
            " [83 14 84]\n",
            " [86  3 87]\n",
            " [89  3 90]\n",
            " [92  5 10]\n",
            " [ 9 94 95]\n",
            " [17  2 98]\n",
            " [12  2  1]\n",
            " [ 2  1  1]\n",
            " [ 3  1  1]\n",
            " [ 1 11  1]\n",
            " [ 1  1  1]\n",
            " [ 2  1  1]\n",
            " [ 1  6  1]\n",
            " [21  8  1]\n",
            " [ 1  8  1]\n",
            " [ 3 15 16]\n",
            " [15 16  1]\n",
            " [ 1 15 16]\n",
            " [ 1  3 25]\n",
            " [ 1  1  1]\n",
            " [ 1  1  1]]\n"
          ]
        }
      ],
      "source": [
        "# Limit the length of the sequences, you will see some sequences get truncated\n",
        "padded = pad_sequences(sequences, maxlen=3)\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnRKDsR197-J"
      },
      "source": [
        "## What happens if some of the sentences contain words that are not in the word index?\n",
        "\n",
        "Here's where the \"out of vocabulary\" token is used. Try generating sequences for some sentences that have words that are not in the word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iqodOpn64c2U",
        "outputId": "0ad27ca4-e992-4795-df50-c2c6555a9c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"my best friend's favorite ice cream flavor is strawberry\", \"my dog's best friend is a manatee\", 'the sun sets beautifully over the horizon', 'we love to travel to exotic destinations', 'she enjoys painting landscapes in her free time', 'his favorite book is a classic novel', 'the garden is blooming with colorful flowers', 'they went on an adventure through the forest', 'I like to relax by the lake on weekends', 'our family enjoys hosting BBQ parties in the backyard', \"my cat's favorite toy is a feather on a string\", 'reading under a tree is very calming', 'the best way to start the day is with a jog', 'she bakes the most delicious cakes', 'he plays guitar in a local band', 'we often have game nights with friends', 'the view from the mountain top is spectacular', 'cooking new recipes is a fun challenge', 'he enjoys building model airplanes', 'our neighbors have the cutest puppy', 'visiting the zoo is always exciting', 'my sister loves collecting vintage postcards', 'watching the stars on a clear night is magical', 'she practices yoga every morning', 'they have a beautiful fish tank in their living room', \"he's passionate about restoring old cars\", 'going to the beach is my favorite summer activity', 'she often writes poetry in her journal', \"the farmer's market has fresh produce every week\", 'their home is filled with unique artwork', 'she loves hiking trails in the national park', \"the city's skyline looks amazing at night\", 'he enjoys photography as a hobby', 'our family reunions are always filled with laughter']\n",
            "<OOV> has the number 1 in the word index.\n",
            "\n",
            "Test Sequence =  [[5, 1, 1, 23, 15, 16, 1, 3, 1], [5, 1, 1, 1, 3, 9, 1], [2, 1, 1, 1, 55, 2, 1], [1, 26, 6, 1, 6, 1, 1], [22, 1, 66, 1, 12, 1, 1, 1], [99, 23, 51, 3, 9, 1, 1], [2, 35, 3, 1, 37, 1, 1], [96, 1, 17, 1, 1, 1, 2, 1], [7, 1, 6, 1, 1, 2, 98, 17, 29], [76, 1, 1, 1, 1, 1, 12, 2, 1], [5, 1, 23, 1, 3, 9, 1, 17, 9, 1], [1, 1, 9, 1, 3, 1, 1], [2, 1, 1, 6, 1, 2, 34, 3, 37, 9, 1], [22, 1, 2, 1, 1, 1], [72, 1, 1, 12, 9, 1, 1], [1, 62, 1, 80, 1, 37, 1], [2, 1, 1, 2, 1, 1, 3, 1], [1, 13, 1, 3, 9, 60, 1], [72, 1, 1, 1, 1], [76, 1, 1, 2, 1, 1], [1, 2, 1, 3, 1, 1], [5, 1, 1, 1, 1, 1], [1, 2, 1, 17, 9, 1, 1, 3, 1], [22, 73, 1, 52, 82], [96, 1, 9, 21, 1, 1, 12, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 6, 2, 1, 3, 5, 23, 1, 61], [22, 62, 1, 1, 12, 1, 1], [2, 1, 1, 1, 39, 1, 52, 53], [1, 1, 3, 1, 37, 1, 1], [22, 1, 47, 1, 12, 2, 1, 1], [2, 1, 1, 36, 1, 1, 1], [72, 1, 1, 1, 9, 95], [76, 1, 1, 41, 1, 1, 37, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[ 0  5  1  1 23 15 16  1  3  1]\n",
            " [ 0  0  0  5  1  1  1  3  9  1]\n",
            " [ 0  0  0  2  1  1  1 55  2  1]\n",
            " [ 0  0  0  1 26  6  1  6  1  1]\n",
            " [ 0  0 22  1 66  1 12  1  1  1]\n",
            " [ 0  0  0 99 23 51  3  9  1  1]\n",
            " [ 0  0  0  2 35  3  1 37  1  1]\n",
            " [ 0  0 96  1 17  1  1  1  2  1]\n",
            " [ 0  7  1  6  1  1  2 98 17 29]\n",
            " [ 0 76  1  1  1  1  1 12  2  1]\n",
            " [ 5  1 23  1  3  9  1 17  9  1]\n",
            " [ 0  0  0  1  1  9  1  3  1  1]\n",
            " [ 1  1  6  1  2 34  3 37  9  1]\n",
            " [ 0  0  0  0 22  1  2  1  1  1]\n",
            " [ 0  0  0 72  1  1 12  9  1  1]\n",
            " [ 0  0  0  1 62  1 80  1 37  1]\n",
            " [ 0  0  2  1  1  2  1  1  3  1]\n",
            " [ 0  0  0  1 13  1  3  9 60  1]\n",
            " [ 0  0  0  0  0 72  1  1  1  1]\n",
            " [ 0  0  0  0 76  1  1  2  1  1]\n",
            " [ 0  0  0  0  1  2  1  3  1  1]\n",
            " [ 0  0  0  0  5  1  1  1  1  1]\n",
            " [ 0  1  2  1 17  9  1  1  3  1]\n",
            " [ 0  0  0  0  0 22 73  1 52 82]\n",
            " [96  1  9 21  1  1 12  1  1  1]\n",
            " [ 0  0  0  0  1  1  1  1  1  1]\n",
            " [ 0  1  6  2  1  3  5 23  1 61]\n",
            " [ 0  0  0 22 62  1  1 12  1  1]\n",
            " [ 0  0  2  1  1  1 39  1 52 53]\n",
            " [ 0  0  0  1  1  3  1 37  1  1]\n",
            " [ 0  0 22  1 47  1 12  2  1  1]\n",
            " [ 0  0  0  2  1  1 36  1  1  1]\n",
            " [ 0  0  0  0 72  1  1  1  9 95]\n",
            " [ 0  0 76  1  1 41  1  1 37  1]]\n"
          ]
        }
      ],
      "source": [
        "# Try turning sentences that contain words that aren't in the word index into sequences.\n",
        "test_data = [\n",
        "    \"my best friend's favorite ice cream flavor is strawberry\",\n",
        "    \"my dog's best friend is a manatee\",\n",
        "    \"the sun sets beautifully over the horizon\",\n",
        "    \"we love to travel to exotic destinations\",\n",
        "    \"she enjoys painting landscapes in her free time\",\n",
        "    \"his favorite book is a classic novel\",\n",
        "    \"the garden is blooming with colorful flowers\",\n",
        "    \"they went on an adventure through the forest\",\n",
        "    \"I like to relax by the lake on weekends\",\n",
        "    \"our family enjoys hosting BBQ parties in the backyard\",\n",
        "    \"my cat's favorite toy is a feather on a string\",\n",
        "    \"reading under a tree is very calming\",\n",
        "    \"the best way to start the day is with a jog\",\n",
        "    \"she bakes the most delicious cakes\",\n",
        "    \"he plays guitar in a local band\",\n",
        "    \"we often have game nights with friends\",\n",
        "    \"the view from the mountain top is spectacular\",\n",
        "    \"cooking new recipes is a fun challenge\",\n",
        "    \"he enjoys building model airplanes\",\n",
        "    \"our neighbors have the cutest puppy\",\n",
        "    \"visiting the zoo is always exciting\",\n",
        "    \"my sister loves collecting vintage postcards\",\n",
        "    \"watching the stars on a clear night is magical\",\n",
        "    \"she practices yoga every morning\",\n",
        "    \"they have a beautiful fish tank in their living room\",\n",
        "    \"he's passionate about restoring old cars\",\n",
        "    \"going to the beach is my favorite summer activity\",\n",
        "    \"she often writes poetry in her journal\",\n",
        "    \"the farmer's market has fresh produce every week\",\n",
        "    \"their home is filled with unique artwork\",\n",
        "    \"she loves hiking trails in the national park\",\n",
        "    \"the city's skyline looks amazing at night\",\n",
        "    \"he enjoys photography as a hobby\",\n",
        "    \"our family reunions are always filled with laughter\"\n",
        "]\n",
        "print (test_data)\n",
        "\n",
        "# Remind ourselves which number corresponds to the out of vocabulary token in the word index\n",
        "print(\"<OOV> has the number\", word_index['<OOV>'], \"in the word index.\")\n",
        "\n",
        "# Convert the test sentences to sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "# Pad the new sequences\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "\n",
        "# Notice that \"1\" appears in the sequence wherever there's a word\n",
        "# that's not in the word index\n",
        "print(padded)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "l09c02_nlp_padding.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}